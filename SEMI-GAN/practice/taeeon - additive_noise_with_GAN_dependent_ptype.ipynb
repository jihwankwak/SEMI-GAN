{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.io import savemat\n",
    "np.set_printoptions(linewidth=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.10 (default, Dec 19 2019, 23:04:32) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise가 X에 dependent 하다고 가정하고 CGAN 학습하는 코드\n",
    "# P-type 만 가지고 .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# excel file 보고 값 기입\n",
    "n_of_cycle = 36 # 조합 갯수 : n_of_cycle\n",
    "n_in_cycle = 50 # random variation 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "#random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, sheet_name, num_of_cycle, num_in_cycle):\n",
    "\n",
    "    X_dim = 3\n",
    "    x_df = pd.read_excel(path, sheet_name=sheet_name, header=0, usecols=\"D:F\")\n",
    "    \n",
    "    X_total = np.zeros((num_of_cycle, X_dim)) \n",
    "    for cycle in range(num_of_cycle):\n",
    "        X_total[cycle] = x_df[cycle*num_in_cycle+1:cycle*num_in_cycle+2].values\n",
    "    X_total_expanded = np.repeat(X_total,num_in_cycle,axis=0)\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------------------------\n",
    "    Y_dim = 9\n",
    "    y_df = pd.read_excel(path, sheet_name=sheet_name, header=0, usecols=\"H:P\")\n",
    "\n",
    "    Y_total = np.zeros((num_of_cycle*num_in_cycle, Y_dim)) \n",
    "    for i in range(num_of_cycle*num_in_cycle):\n",
    "        Y_total[i] = y_df[i+1:i+2].values\n",
    "    \n",
    "    return (X_total, X_total_expanded, Y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4) into shape (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-85b993d9f3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 파일로부터 데이터불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_total_expanded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_total_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./LER_20200529_V004_edit.xlsx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Generated DATAs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_in_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_in_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_total_expanded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_total_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0modd_index_for_ptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_of_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-6b4200f5de1a>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, sheet_name, num_of_cycle, num_in_cycle)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_cycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mX_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_in_cycle\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcycle\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_in_cycle\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mX_total_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_in_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4) into shape (3)"
     ]
    }
   ],
   "source": [
    "# 파일로부터 데이터불러오기\n",
    "X_total, X_total_expanded, Y_total_= load_data(path='./LER_20200529_V004_edit.xlsx', sheet_name='Generated DATAs', num_of_cycle=72, num_in_cycle=n_in_cycle)\n",
    "print(X_total.shape, X_total_expanded.shape, Y_total_.shape)\n",
    "\n",
    "odd_index_for_ptype = np.array([2*i+1 for i in range(n_of_cycle)])\n",
    "X_total = X_total[odd_index_for_ptype]\n",
    "X_total_expanded = np.repeat(X_total,n_in_cycle,axis=0)\n",
    "\n",
    "Y_total = np.zeros((n_of_cycle*n_in_cycle, Y_total_.shape[1]))\n",
    "for i in range(len(odd_index_for_ptype)):\n",
    "    Y_total[i*n_in_cycle:(i+1)*n_in_cycle]=Y_total_[odd_index_for_ptype[i]*n_in_cycle:(odd_index_for_ptype[i]+1)*n_in_cycle]\n",
    "\n",
    "\n",
    "\n",
    "X_data = copy.deepcopy(X_total)\n",
    "print(X_data.shape)\n",
    "\n",
    "y_data = copy.deepcopy(Y_total)\n",
    "print(y_data.shape)\n",
    "print('y값 nan 있는 곳 있나? : ', np.argwhere(np.isnan(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 4) (3600, 4) (3600, 9)\n",
      "(36, 4)\n",
      "(1800, 9)\n",
      "y값 nan 있는 곳 있나? :  []\n"
     ]
    }
   ],
   "source": [
    "# 파일로부터 데이터불러오기\n",
    "X_total, X_total_expanded, Y_total_= load_data(path='./2020_LER_20200529_V004.xlsx', sheet_name='uniformly sampling', num_of_cycle=72, num_in_cycle=n_in_cycle)\n",
    "print(X_total.shape, X_total_expanded.shape, Y_total_.shape)\n",
    "\n",
    "odd_index_for_ptype = np.array([2*i+1 for i in range(n_of_cycle)])\n",
    "X_total = X_total[odd_index_for_ptype]\n",
    "X_total_expanded = np.repeat(X_total,n_in_cycle,axis=0)\n",
    "\n",
    "Y_total = np.zeros((n_of_cycle*n_in_cycle, Y_total_.shape[1]))\n",
    "for i in range(len(odd_index_for_ptype)):\n",
    "    Y_total[i*n_in_cycle:(i+1)*n_in_cycle]=Y_total_[odd_index_for_ptype[i]*n_in_cycle:(odd_index_for_ptype[i]+1)*n_in_cycle]\n",
    "\n",
    "\n",
    "\n",
    "X_data = copy.deepcopy(X_total)\n",
    "print(X_data.shape)\n",
    "\n",
    "y_data = copy.deepcopy(Y_total)\n",
    "print(y_data.shape)\n",
    "print('y값 nan 있는 곳 있나? : ', np.argwhere(np.isnan(y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 9)\n",
      "[ 2.63792000e-12  2.34130000e-05  6.97438000e-06  5.77404800e-07  2.20184000e-05 -3.50360000e-01 -3.63020000e-01 -6.24891600e+01 -1.97230769e+01]\n",
      "[ 3.93564000e-12  2.63074000e-05  7.70604000e-06  7.46248000e-07  2.47592000e-05 -3.39000000e-01 -3.51820000e-01 -6.20826800e+01 -1.75076923e+01]\n"
     ]
    }
   ],
   "source": [
    "# 1. mean 예측하는 모델만들어야함.\n",
    "y_mean = np.mean(np.split(y_data, n_of_cycle), axis = 1)\n",
    "#y_std = np.std(np.split(y_data, n_of_cycle), axis = 1)\n",
    "print(y_mean.shape)\n",
    "\n",
    "# just check\n",
    "print(np.min(y_mean,axis=0))\n",
    "print(np.max(y_mean,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.92828000e-12  2.59490000e-05  7.49488000e-06 ... -3.53800000e-01 -6.21401400e+01 -1.81230769e+01]\n",
      " [ 2.92828000e-12  2.59490000e-05  7.49488000e-06 ... -3.53800000e-01 -6.21401400e+01 -1.81230769e+01]\n",
      " [ 2.92828000e-12  2.59490000e-05  7.49488000e-06 ... -3.53800000e-01 -6.21401400e+01 -1.81230769e+01]\n",
      " ...\n",
      " [ 2.88569400e-12  2.45318000e-05  7.24998000e-06 ... -3.58820000e-01 -6.21613600e+01 -1.88000000e+01]\n",
      " [ 2.88569400e-12  2.45318000e-05  7.24998000e-06 ... -3.58820000e-01 -6.21613600e+01 -1.88000000e+01]\n",
      " [ 2.88569400e-12  2.45318000e-05  7.24998000e-06 ... -3.58820000e-01 -6.21613600e+01 -1.88000000e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Y-Y_mean을 통해 noise만 만들어낼꺼임.. shape 맞추기\n",
    "y_mean_expanded = np.repeat(y_mean, n_in_cycle, axis=0)\n",
    "noise_total = y_data - y_mean_expanded\n",
    "print(y_mean_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cycle을 split하자\n",
    "train_num = 25\n",
    "val_num = 5\n",
    "test_num = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 5 6\n",
      "25 5 6\n"
     ]
    }
   ],
   "source": [
    "#데이터 스플릿\n",
    "X_train = X_data[:train_num]\n",
    "X_val = X_data[train_num:train_num+val_num]\n",
    "X_test = X_data[train_num+val_num: train_num + val_num + test_num]\n",
    "\n",
    "y_train = y_data[:train_num*n_in_cycle]\n",
    "y_val = y_data[train_num*n_in_cycle:(train_num+val_num)*n_in_cycle]\n",
    "y_test = y_data[(train_num+val_num)*n_in_cycle : (train_num + val_num + test_num)*n_in_cycle]\n",
    "\n",
    "y_mean_train = y_mean[:train_num]\n",
    "y_mean_val = y_mean[train_num:train_num+val_num]\n",
    "y_mean_test = y_mean[train_num+val_num: train_num + val_num + test_num]\n",
    "\n",
    "print(len(X_train), len(X_val), len(X_test))\n",
    "print(len(y_mean_train), len(y_mean_val), len(y_mean_test))\n",
    "\n",
    "noise_train = noise_total[:train_num*n_in_cycle]\n",
    "noise_val = noise_total[train_num*n_in_cycle:(train_num+val_num)*n_in_cycle]\n",
    "noise_test = noise_total[(train_num+val_num)*n_in_cycle: (train_num + val_num + test_num)*n_in_cycle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5b977d560264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pd = pd.DataFrame({'Column0': y_train[:, 0], 'Column1': y_train[:, 1], 'Column2': y_train[:, 2],\n",
    "                             'Column3': y_train[:, 3], 'Column4': y_train[:, 4], 'Column5': y_train[:, 5],\n",
    "                            'Column6': y_train[:, 6], 'Column7': y_train[:, 7], 'Column8': y_train[:, 8]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(y_train_pd.values)\n",
    "y_train_pd['pca-one'] = pca_result[:,0]\n",
    "y_train_pd['pca-two'] = pca_result[:,1] \n",
    "y_train_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_for_coloring = np.zeros(len(y_train_pd), dtype=int)\n",
    "for i in range(train_num):\n",
    "    for j in range(n_in_cycle):\n",
    "        cycle_for_coloring[n_in_cycle*i + j] = i\n",
    "y_train_pd['cycle'] = cycle_for_coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    current_cycle = i\n",
    "    # 2-dim visualization of noise\n",
    "    flatui = [\"red\"]#, \"#9b59b6\"]#, \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\", \n",
    "        palette=sns.color_palette(flatui),\n",
    "        data=y_train_pd.loc[current_cycle*50:current_cycle*50+50-1,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    plt.xlim(-20,20)\n",
    "    plt.ylim(-5,5)\n",
    "    plt.title('y_train for cycle : '+str(i), fontsize=15)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean_train_pd = pd.DataFrame({'Column0': y_mean_train[:, 0], 'Column1': y_mean_train[:, 1], 'Column2': y_mean_train[:, 2],\n",
    "                             'Column3': y_mean_train[:, 3], 'Column4': y_mean_train[:, 4], 'Column5': y_mean_train[:, 5],\n",
    "                            'Column6': y_mean_train[:, 6], 'Column7': y_mean_train[:, 7], 'Column8': y_mean_train[:, 8]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(y_mean_train_pd.values)\n",
    "y_mean_train_pd['pca-one'] = pca_result[:,0]\n",
    "y_mean_train_pd['pca-two'] = pca_result[:,1] \n",
    "y_mean_train_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_for_coloring = np.zeros(len(y_mean_train_pd), dtype=int)\n",
    "for i in range(train_num):\n",
    "    cycle_for_coloring[i] = i\n",
    "y_mean_train_pd['cycle'] = cycle_for_coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-dim visualization of noise\n",
    "number_of_colors = np.int_(np.max(y_mean_train_pd['cycle'].values))+1\n",
    "plt.figure(figsize=(12,15))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"cycle\",\n",
    "    palette=sns.color_palette(\"Paired\", number_of_colors),\n",
    "    data=y_mean_train_pd.loc[:,:],\n",
    "    legend=\"full\",\n",
    "    alpha=1.0\n",
    ")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('y_mean_train', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Noise (PCA+TSNE to 2dim / PCA+TSNE to 3dim / UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_train_pd = pd.DataFrame({'Column0': noise_train[:, 0], 'Column1': noise_train[:, 1], 'Column2': noise_train[:, 2],\n",
    "                             'Column3': noise_train[:, 3], 'Column4': noise_train[:, 4], 'Column5': noise_train[:, 5],\n",
    "                            'Column6': noise_train[:, 6], 'Column7': noise_train[:, 7], 'Column8': noise_train[:, 8]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(noise_train_pd.values)\n",
    "noise_train_pd['pca-one'] = pca_result[:,0]\n",
    "noise_train_pd['pca-two'] = pca_result[:,1] \n",
    "noise_train_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_for_coloring = np.zeros(len(noise_train_pd), dtype=int)\n",
    "for i in range(train_num):\n",
    "    for j in range(n_in_cycle):\n",
    "        cycle_for_coloring[n_in_cycle*i + j] = i\n",
    "noise_train_pd['cycle'] = cycle_for_coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_train_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-dim visualization of noise\n",
    "number_of_colors = np.int_(np.max(noise_train_pd['cycle'].values))+1\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"cycle\",\n",
    "    palette=sns.color_palette(\"Paired\", number_of_colors),\n",
    "    data=noise_train_pd.loc[:,:],\n",
    "    legend=\"full\",\n",
    "    alpha=1.0\n",
    ")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.title('noise_train', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    current_cycle = i\n",
    "    # 2-dim visualization of noise\n",
    "    flatui = [\"red\"]#, \"#9b59b6\"]#, \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\",\n",
    "        palette=sns.color_palette(flatui),\n",
    "        data=noise_train_pd.loc[current_cycle*25:current_cycle*25+25-1,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    plt.xlim(-6,6)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title('noise for cycle : '+str(i), fontsize=15)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noise가 X에 dependent 하는 것 같기도 하다 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-dimensional visualization\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=noise_train_pd.loc[:,:][\"pca-one\"], \n",
    "    ys=noise_train_pd.loc[:,:][\"pca-two\"], \n",
    "    zs=noise_train_pd.loc[:,:][\"pca-three\"], \n",
    "    c=noise_train_pd.loc[:,:][\"cycle\"], \n",
    "    cmap='tab20'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "scaled_noise_train_pd = StandardScaler().fit_transform(noise_train_pd)\n",
    "embedding = reducer.fit_transform(scaled_noise_train_pd)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=[sns.color_palette(\"Set1\", n_colors=50, desat=.9)[x] for x in noise_train_pd.cycle.values])\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('UMAP projection of noise train data', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalizing(data, params):\n",
    "#     tr_min, tr_max = params\n",
    "#     normalized = (data-tr_min)/(tr_max-tr_min)\n",
    "#     return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1fb4033b4d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 정규화 계수 구하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#params = (np.min(y_mean_var_train, axis=0), np.max(y_mean_var_train, axis=0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtr_x_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtr_x_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtr_y_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_mean_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# 정규화 계수 구하기\n",
    "#params = (np.min(y_mean_var_train, axis=0), np.max(y_mean_var_train, axis=0))\n",
    "tr_x_mean = np.mean(X_train, axis=0, dtype=np.float32) \n",
    "tr_x_std = np.std(X_train, axis=0, dtype=np.float32)\n",
    "tr_y_mean = np.mean(y_mean_train, axis=0, dtype=np.float32) \n",
    "tr_y_std = np.std(y_mean_train, axis=0, dtype=np.float32)\n",
    "print(tr_x_mean, tr_x_std)\n",
    "print(tr_y_mean, tr_y_std)\n",
    "#tr_y_mean[0:6] = np.mean(y_data[:2300], axis=0, dtype=np.float32) \n",
    "#tr_y_std[0:6] = np.std(y_data[:2300], axis=0, dtype=np.float32)\n",
    "#print(tr_x_mean, tr_x_std)\n",
    "#print(tr_y_mean, tr_y_std)\n",
    "\n",
    "# 데이터 정규화\n",
    "X_train_normalized = (X_train - tr_x_mean) / tr_x_std\n",
    "X_val_normalized = (X_val - tr_x_mean) / tr_x_std\n",
    "X_test_normalized = (X_test - tr_x_mean) / tr_x_std\n",
    "y_mean_train_normalized = (y_mean_train - tr_y_mean) / tr_y_std\n",
    "y_mean_val_normalized = (y_mean_val - tr_y_mean) / tr_y_std\n",
    "y_mean_test_normalized = (y_mean_test - tr_y_mean) / tr_y_std\n",
    "\n",
    "# y_mean_var_train_norm = normalizing(y_mean_var_train, params)\n",
    "# y_mean_var_val_norm = normalizing(y_mean_var_val, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NN predicting y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        #x=F.dropout(x, training=self.training)\n",
    "        x=self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, scheduler, epochs=100, verbose=True):\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    #optimizer = optim.SGD(model.parameters(), lr = 0.00001)\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    r2_score_list = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_num=0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            train_x, train_y = data\n",
    "            train_x, train_y = train_x.to(device), train_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(train_x)\n",
    "            loss = criterion(output, train_y) #순서맞음\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss\n",
    "            \n",
    "            train_num += len(train_y)\n",
    "            \n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_num = 0\n",
    "        \n",
    "        val_target = torch.tensor([]).float().to(device)\n",
    "        val_pred = torch.tensor([]).float().to(device)\n",
    "        \n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_out=[]\n",
    "            with torch.no_grad():\n",
    "                val_x, val_y = data\n",
    "                val_x, val_y = val_x.to(device), val_y.to(device)\n",
    "                \n",
    "                val_output = model(val_x)\n",
    "                val_loss += criterion(val_output, val_y)\n",
    "\n",
    "                val_target = torch.cat((val_target, val_y))\n",
    "                val_pred = torch.cat((val_pred, val_output))\n",
    "                \n",
    "                val_num += len(val_y)\n",
    "\n",
    "        \n",
    "        \n",
    "        train_loss /= train_num\n",
    "        val_loss /= val_num   \n",
    "        r2 = r2_score(val_target, val_pred) # 순서맞음\n",
    "        \n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        r2_score_list.append(r2)\n",
    "        \n",
    "        for param_group in optimizer.param_groups:  #scheduler\n",
    "            lr = param_group['lr']\n",
    "            #print('lr_schedule:',lr)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if verbose==True :\n",
    "            if((epoch+1)% 10 == 0):\n",
    "                print(\"epoch:{:2d}, lr:{:.6f}, || train_loss:{:.6f}, val_loss:{:.6f}, r2_score:{:.6f}\"\n",
    "              .format(epoch, lr, train_loss, val_loss, r2))\n",
    "    \n",
    "    if verbose==False :        \n",
    "        print(\"final results : epoch:{:2d}, lr:{:.6f}, || train_loss:{:.6f}, val_loss:{:.6f}, r2_score:{:.6f}\"\n",
    "              .format(epoch, lr, train_loss, val_loss, r2))        \n",
    "    return model, train_loss_list, val_loss_list, r2_score_list\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor로..\n",
    "X_train_normalized = torch.tensor(X_train_normalized).float()\n",
    "X_val_normalized = torch.tensor(X_val_normalized).float()\n",
    "y_mean_train_normalized = torch.tensor(y_mean_train_normalized).float()\n",
    "y_mean_val_normalized = torch.tensor(y_mean_val_normalized).float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_normalized, y_mean_train_normalized)\n",
    "val_dataset = TensorDataset(X_val_normalized, y_mean_val_normalized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net(hidden_dim=100)\n",
    "# model.apply(init_normal)\n",
    "# #device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "# model.to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 5e-5)\n",
    "# #optimizer = optim.SGD(model.parameters(), lr = 5e-4, momentum=0.9)\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5) # scheduler\n",
    "# model, train_loss_list, val_loss_list, r2_scores = train(model, train_loader, val_loader,optimizer, exp_lr_scheduler, \n",
    "#                                                          epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(hidden_dim=100)\n",
    "model.apply(init_normal)\n",
    "#device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device='cpu'\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 5e-5)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = 5e-4, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5) # scheduler\n",
    "model, train_loss_list, val_loss_list, r2_scores = train(model, train_loader, val_loader,optimizer, exp_lr_scheduler, \n",
    "                                                         epochs=1000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"train loss\")\n",
    "plt.plot(np.array(train_loss_list), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"val loss\")\n",
    "plt.plot(np.array(val_loss_list), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"r2_score\")\n",
    "plt.ylim(-2,1)\n",
    "plt.plot(np.array(r2_scores),'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Y_mean (with inverting normalization constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical mean 보다는 좀 낫다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized = torch.tensor(X_test_normalized).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(X_test_normalized.to(device))\n",
    "output = np.array(pred.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output inverse 연산 (최종 예측한 Y_mean)\n",
    "print(tr_x_mean)\n",
    "print(tr_x_std)\n",
    "print(tr_y_mean)\n",
    "print(tr_y_std)\n",
    "\n",
    "mean_output = output*tr_y_std + tr_y_mean            #(tr_max - tr_min) + tr_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize prediction vs target for Y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean_test_pd = pd.DataFrame({'Column0': y_mean_test[:, 0], 'Column1': y_mean_test[:, 1], 'Column2': y_mean_test[:, 2],\n",
    "                             'Column3': y_mean_test[:, 3], 'Column4': y_mean_test[:, 4], 'Column5': y_mean_test[:, 5],\n",
    "                            'Column6': y_mean_test[:, 6], 'Column7': y_mean_test[:, 7], 'Column8': y_mean_test[:, 8]\n",
    "                              })\n",
    "y_mean_test_predicted_pd = pd.DataFrame({'Column0': mean_output[:, 0], 'Column1': mean_output[:, 1], 'Column2': mean_output[:, 2],\n",
    "                             'Column3': mean_output[:, 3], 'Column4': mean_output[:, 4], 'Column5': mean_output[:, 5],\n",
    "                            'Column6': mean_output[:, 6], 'Column7': mean_output[:, 7], 'Column8': mean_output[:, 8]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(y_mean_test_pd.values)\n",
    "y_mean_test_pd['pca-one'] = pca_result[:,0]\n",
    "y_mean_test_pd['pca-two'] = pca_result[:,1] \n",
    "y_mean_test_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(y_mean_test_predicted_pd.values)\n",
    "y_mean_test_predicted_pd['pca-one'] = pca_result[:,0]\n",
    "y_mean_test_predicted_pd['pca-two'] = pca_result[:,1] \n",
    "y_mean_test_predicted_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_for_coloring = np.zeros(len(y_mean_test_pd), dtype=int)\n",
    "for i in range(test_num):\n",
    "    cycle_for_coloring[i] = i\n",
    "y_mean_test_pd['cycle'] = cycle_for_coloring\n",
    "y_mean_test_predicted_pd['cycle'] = cycle_for_coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    current_cycle = i\n",
    "    # 2-dim visualization of noise\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\",\n",
    "        palette=sns.color_palette([\"red\"]),\n",
    "        data=y_mean_test_pd.loc[current_cycle:current_cycle,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\",\n",
    "        palette=sns.color_palette([\"blue\"]),\n",
    "        data=y_mean_test_predicted_pd.loc[current_cycle:current_cycle,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    \n",
    "    plt.xlim(-2,2)\n",
    "    plt.ylim(-0.5,0.5)\n",
    "    plt.title('y_mean for cycle : '+str(i), fontsize=15)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Noise with GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(batch_size = 1, d_noise=100):    # latent dim 크기를 가지는 가우시안 노이즈 벡터를 배치수만큼의 갯수로 추출하는 함수.\n",
    "    return torch.randn(batch_size, d_noise, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G(nn.Module):\n",
    "    def __init__(self, d_noise, d_hidden, dim_output):\n",
    "        super(G, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_noise, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_hidden)\n",
    "        self.fc3 = nn.Linear(d_hidden, dim_output)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "        x=F.relu(self.fc1(x))\n",
    "        #x=F.dropout(x, p=0.1)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        #x=F.dropout(x, p=0.1)\n",
    "        x=self.fc3(x)\n",
    "        output = x\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class D(nn.Module):\n",
    "    def __init__(self, dim_output, d_hidden):\n",
    "        super(D, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_output, d_hidden)\n",
    "        self.fc2 = nn.Linear(d_hidden, d_hidden)\n",
    "        self.fc3 = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "        x=F.relu(self.fc1(x))\n",
    "        #x=F.dropout(x, p=0.1)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        #x=F.dropout(x, p=0.1)\n",
    "        x=torch.sigmoid(self.fc3(x))\n",
    "        output = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = 10\n",
    "dim_output = 9\n",
    "d_hidden = 10\n",
    "\n",
    "#CONDITIONAL\n",
    "G = G(dim_input+3, d_hidden, dim_output)\n",
    "D = D(dim_output+3, d_hidden)\n",
    "            \n",
    "\n",
    "G.to(device)\n",
    "D.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GANs 학습에 사용되는 함수\n",
    "\n",
    "\n",
    "def run_epoch(generator, discriminator, _optimizer_g, _optimizer_d, train_data_loader):\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    for real_batch, X_batch in train_data_loader:\n",
    "        real_batch, X_batch = real_batch.to(device), X_batch.to(device)\n",
    "        \n",
    "        batch_size=len(real_batch)\n",
    "        \n",
    "        input_for_D = torch.cat((real_batch,X_batch),axis=1)\n",
    "        input_for_G = torch.cat((sample_z(batch_size, dim_input),X_batch),axis=1)\n",
    "        \n",
    "        p_real = discriminator(input_for_D)\n",
    "        p_fake = discriminator(torch.cat((generator(input_for_G), X_batch),axis=1))\n",
    "\n",
    "\n",
    "        loss_real = -1 * torch.log(p_real)   # -1 for gradient ascending\n",
    "        loss_fake = -1 * torch.log(1.-p_fake) # -1 for gradient ascending\n",
    "        loss_d    = (loss_real + loss_fake).mean() # 배치에 대해 mean\n",
    "        \n",
    "        \n",
    "        # Update parameters\n",
    "        _optimizer_d.zero_grad() #grad 계산 전 항상 init 해주기\n",
    "        loss_d.backward() # grad 계산\n",
    "        _optimizer_d.step() # update\n",
    "\n",
    "        \n",
    "        input_for_G = torch.cat((sample_z(batch_size, dim_input),X_batch),axis=1)\n",
    "        p_fake = discriminator(torch.cat((generator(input_for_G), X_batch),axis=1))\n",
    "                \n",
    "        # instead of: torch.log(1.-p_fake).mean() <- explained in Section 3\n",
    "        loss_g = -1 * torch.log(p_fake).mean()  # log(D(G(z)))를 maximize 하는 것으로 (초기 D의 overpower 문제 해결위해)\n",
    "        \n",
    "        # Update parameters\n",
    "        _optimizer_g.zero_grad() #grad 계산 전 항상 init 해주기\n",
    "        loss_g.backward() #grad 계산\n",
    "        _optimizer_g.step()\n",
    "          \n",
    "def evaluate_model(generator, discriminator, test_data_loader):\n",
    "    \n",
    "    p_real, p_fake = 0.,0.\n",
    "    \n",
    "    generator.eval() # 모드 변경\n",
    "    discriminator.eval() # 모드 변경\n",
    "    \n",
    "    batch_num=0\n",
    "    for real_batch, X_batch in test_data_loader:\n",
    "        \n",
    "        real_batch, X_batch = real_batch.to(device), X_batch.to(device)\n",
    "        batch_size = len(real_batch)\n",
    "        \n",
    "        with torch.autograd.no_grad():\n",
    "            input_for_D = torch.cat((real_batch,X_batch),axis=1)\n",
    "            input_for_G = torch.cat((sample_z(batch_size, dim_input),X_batch),axis=1)\n",
    "            p_real += torch.sum(discriminator(input_for_D)).item()/len(real_batch)\n",
    "            p_fake += torch.sum(discriminator(torch.cat((generator(input_for_G), X_batch),axis=1))).item()/len(real_batch)\n",
    "            \n",
    "        batch_num += 1\n",
    "    p_real /= batch_num\n",
    "    p_fake /= batch_num\n",
    "    return p_real, p_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-3352ae442eae>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3352ae442eae>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tr_noise_std = np.std(noise_train, axis=0, dtype=np.floa t32)\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tr_noise_mean = np.mean(noise_train, axis=0, dtype=np.float32) \n",
    "tr_noise_std = np.std(noise_train, axis=0, dtype=np.floa t32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_train_normalized = (noise_train-tr_noise_mean)/tr_noise_std\n",
    "noise_val_normalized = (noise_val-tr_noise_mean)/tr_noise_std\n",
    "noise_test_normalized = (noise_test-tr_noise_mean)/tr_noise_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_GAN = TensorDataset(torch.tensor(noise_train_normalized).float(),torch.tensor(np.repeat(X_train_normalized.numpy(), 50, axis=0)).float())\n",
    "val_dataset_GAN = TensorDataset(torch.tensor(noise_val_normalized).float(),torch.tensor(np.repeat(X_val_normalized.numpy(), 50, axis=0)).float())\n",
    "\n",
    "train_loader_GAN = DataLoader(train_dataset_GAN, batch_size = 64, shuffle = True)\n",
    "val_loader_GAN = DataLoader(val_dataset_GAN, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = torch.optim.Adam(G.parameters(), lr = 0.0001)\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr = 0.0005)\n",
    "\n",
    "p_real_trace = []\n",
    "p_fake_trace = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(model): # weight initialization 인듯\n",
    "    for p in model.parameters():\n",
    "        if(p.dim() > 1):\n",
    "            nn.init.xavier_normal_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p, 0.1, 0.2)   \n",
    "\n",
    "init_params(G)\n",
    "init_params(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_GAN = lr_scheduler.StepLR(optimizer_d, step_size=50, gamma=0.5) # scheduler\n",
    "\n",
    "for epoch in range(200):\n",
    "    \n",
    "    #print('epoch', epoch)\n",
    "    \n",
    "    run_epoch(G, D, optimizer_g, optimizer_d, train_loader_GAN)\n",
    "    p_real, p_fake = evaluate_model(G, D, val_loader_GAN)\n",
    "    \n",
    "    p_real_trace.append(p_real)\n",
    "    p_fake_trace.append(p_fake) \n",
    "    \n",
    "    \n",
    "    for param_group in optimizer_d.param_groups:  #scheduler\n",
    "        lr_d = param_group['lr']\n",
    "        #print('lr_schedule:',lr)\n",
    "    scheduler_GAN.step()\n",
    "    \n",
    "    \n",
    "    if((epoch+1)% 10 == 0):\n",
    "        print('epoch : (%i/200), lr_d:%.10f, p_real: %f, p_g: %f' % (epoch+1, lr_d, p_real, p_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch이 진행됨에 따라 D(x)와 D(G(z))의 확률값은 어떻게 개선되어 가는지?\n",
    "plt.plot(p_fake_trace, label='D(x_generated)')\n",
    "plt.plot(p_real_trace, label='D(x_real)')\n",
    "plt.legend(bbox_to_anchor=(0.60, 0.95), loc=2, borderaxespad=0.)\n",
    "\n",
    "#plt.savefig(name+'_G&D_probability.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize prediction vs target for Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_G = torch.cat((sample_z(len(noise_test), dim_input),torch.from_numpy(np.repeat(X_test_normalized.numpy(), 50, axis=0))),axis=1)\n",
    "noise_test_normalized_predicted = G(input_for_G).detach().cpu().numpy()\n",
    "noise_test_predicted = noise_test_normalized_predicted*tr_noise_std+tr_noise_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_test_pd = pd.DataFrame({'Column0': noise_test[:, 0], 'Column1': noise_test[:, 1], 'Column2': noise_test[:, 2],\n",
    "                             'Column3': noise_test[:, 3], 'Column4': noise_test[:, 4], 'Column5': noise_test[:, 5],\n",
    "                            'Column6': noise_test[:, 6], 'Column7': noise_test[:, 7], 'Column8': noise_test[:, 8]\n",
    "                              })\n",
    "\n",
    "noise_test_predicted_pd = pd.DataFrame({'Column0': noise_test_predicted[:, 0], 'Column1': noise_test_predicted[:, 1], 'Column2': noise_test_predicted[:, 2],\n",
    "                             'Column3': noise_test_predicted[:, 3], 'Column4': noise_test_predicted[:, 4], 'Column5': noise_test_predicted[:, 5],\n",
    "                            'Column6': noise_test_predicted[:, 6], 'Column7': noise_test_predicted[:, 7], 'Column8': noise_test_predicted[:, 8]\n",
    "                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(noise_test_pd.values)\n",
    "noise_test_pd['pca-one'] = pca_result[:,0]\n",
    "noise_test_pd['pca-two'] = pca_result[:,1] \n",
    "noise_test_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(noise_test_predicted_pd.values)\n",
    "noise_test_predicted_pd['pca-one'] = pca_result[:,0]\n",
    "noise_test_predicted_pd['pca-two'] = pca_result[:,1] \n",
    "noise_test_predicted_pd['pca-three'] = pca_result[:,2]\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_for_coloring = np.zeros(len(noise_test_pd), dtype=int)\n",
    "for i in range(test_num):\n",
    "    for j in range(n_in_cycle):\n",
    "        cycle_for_coloring[n_in_cycle*i + j] = i\n",
    "noise_test_pd['cycle'] = cycle_for_coloring\n",
    "noise_test_predicted_pd['cycle'] = cycle_for_coloring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    current_cycle = i\n",
    "    # 2-dim visualization of noise\n",
    "    flatui = [\"red\"]#, \"#9b59b6\"]#, \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\",\n",
    "        palette=sns.color_palette(['red']),\n",
    "        data=noise_test_pd.loc[current_cycle*50:current_cycle*50+50-1,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        x=\"pca-one\", y=\"pca-two\",\n",
    "        hue=\"cycle\",\n",
    "        palette=sns.color_palette(['blue']),\n",
    "        data=noise_test_predicted_pd.loc[current_cycle*50:current_cycle*50+50-1,:],\n",
    "        legend=\"full\",\n",
    "        alpha=1.0\n",
    "    )\n",
    "    plt.xlim(-6,6)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.title('noise for cycle : '+str(i), fontsize=15)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_generated = np.repeat(mean_output, 50, axis=0)+ noise_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_generated[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_list = {'y_test_generated':y_test_generated, 'y_test':y_test}\n",
    "saving_list = dict(saving_list)\n",
    "savemat('./dependent_noise_results.mat', saving_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('dependent_noise_generated_sample.txt', y_test_generated, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()\n",
    "print(mse(y_test_generated, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test[:50,0],y_test[:50,5])\n",
    "plt.scatter(y_test_generated[:50,0],y_test[:50,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test[50:100,0],y_test[50:100,5])\n",
    "plt.scatter(y_test_generated[50:100,0],y_test[50:100,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test[200:250,0],y_test[200:250,5])\n",
    "plt.scatter(y_test_generated[200:250,0],y_test[200:250,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model(X_train_normalized.to(device))\n",
    "output_train = np.array(pred_train.tolist())\n",
    "mean_output_train = output_train*tr_y_std + tr_y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_for_G = torch.cat((sample_z(len(noise_train), dim_input), torch.from_numpy(np.repeat(X_train_normalized.numpy(), 50, axis=0))),axis=1)\n",
    "noise_train_normalized_predicted = G(input_for_G).detach().cpu().numpy()\n",
    "noise_train_predicted = noise_train_normalized_predicted*tr_noise_std+tr_noise_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_predicted = np.repeat(mean_output_train,50,axis=0)+noise_train_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train[50:100,0],y_train[50:100,5])\n",
    "plt.scatter(Y_train_predicted[50:100,0],Y_train_predicted[50:100,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train[150:200,0],y_train[150:200,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.xlabel('Ioff,sat', fontsize=20)\n",
    "plt.ylabel('Vt,sat', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train[150:200,2],y_train[150:200,6])\n",
    "plt.xlim(1e-5,13e-12)\n",
    "plt.xlabel('Id,lin', fontsize=20)\n",
    "plt.ylabel('Vt,lin', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train[150:200,0],y_train[150:200,5])\n",
    "plt.scatter(Y_train_predicted[150:200,0],Y_train_predicted[150:200,5])\n",
    "plt.xlim(1e-12,13e-12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(‘default’)\n",
    "for i in range(30):\n",
    "    plt.figure()\n",
    "    plt.scatter(Y_test[i*10:i*10+10][:,0], Y_test[i*10:i*10+10][:,2], label = ‘True’)\n",
    "    plt.scatter(Y_output[i*10:i*10+10][:,0], Y_output[i*10:i*10+10][:,2], label = ‘Generated(BNN)‘)\n",
    "    plt.title(r’$\\sigma: %.4f, \\xi_x: %.4f, \\xi_y: %.4f, \\alpha:%.4f$’%(X_all[i*10][0],X_all[i*10][1],X_all[i*10][2],X_all[i*10][3]))\n",
    "    plt.xlim(1e-12,13e-12)\n",
    "    plt.xlabel(‘Ioff,sat’, fontsize=20)\n",
    "    plt.ylabel(‘Vt,sat’, fontsize=20)\n",
    "    plt.legend(loc=‘lower left’, fontsize=15)\n",
    "    if not os.path.exists(‘./figures’):\n",
    "        os.makedirs(‘./figures’)\n",
    "    plt.savefig(‘figures/Linear_%d.pdf’%i, bbox_inches=‘tight’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V_T_sat\n",
    "# I_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional VAE\n",
    "# Bayesian Linear Regressison -> multivariate x\n",
    "# Target을 mean, cov (9 by 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CGAN, WGAN\n",
    "# infoGAN (실험 - attribute 바꿔보면서 의도대로 .. 각 attribute 잘 학습하지 않을까 ?)\n",
    "# interpolation ..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
